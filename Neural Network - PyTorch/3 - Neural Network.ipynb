{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   #Most of the times needs parameters passed to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train  = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train  = False, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- self.fc1 = nn.Linear(28*28, 64) \n",
    "  - input here is the image, but we cannot pass the image directly so we need to pass the flattened image\n",
    "  - output - we are building 3 hidden layers with 64 neurons each\n",
    "  - Linear - fully connected - flat and linear\n",
    "  - Conv - for Convolutional neural network\n",
    "  \n",
    "  \n",
    "- self.fc2 = nn.Linear(64, 64)\n",
    "  - input 64 from previous layer,\n",
    "  - output can be variable but is set to 64\n",
    "  \n",
    "  \n",
    "- self.fc4 = nn.Linear(64, 10)\n",
    "  - 10 as the output as we have 10 digits to be classified \n",
    "  \n",
    "  \n",
    "A simple neural network is the one in which the data passes in one direction - feed forward \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) \n",
    "        self.fc2 = nn.Linear(64, 64) \n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):  # x -input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "                                        \n",
    "net = Net()\n",
    "print(net)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.rand((28,28))\n",
    "# X = X.view(-1, 28*28) # -1/1 specifies that the input is of an unknown shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss - measure of how wrong the model is\n",
    "\n",
    "\n",
    "- Optimizer - go through and adjust the weights based on the loss so as to lessen the loss over time. This time depends upon the learning rate.\n",
    "\n",
    "\n",
    "- Decaying learning rate\n",
    "\n",
    "\n",
    "- epoch - whole pass through the data once\n",
    "\n",
    "\n",
    "- If the data is a scalar value, use nll_loss\n",
    "- If the data is a one hot vector use mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2067, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) #1e-3\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a batch of featureset and labels\n",
    "        X, y = data\n",
    "#         print(X[0])\n",
    "#         print(y[0])\n",
    "#         break\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.968\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():   #We dn't want to calculate the gradients here, \n",
    "    for data in testset: #We just want to check how good the network is\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))\n",
    "      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADbNJREFUeJzt3X+MHHUZx/HP0+tx1RaxFVtrqS3FClaUYs7yS00NQUFJClEbqiE1oocgRhCNtSZKTEgKCP4I/kix1ZqUXwkgRRulqZhqaBoOKFCsKGJLr609sBWK0B/XPv5xc+Qot9/d7s7O7PV5v5Jmd+eZ2Xmy6edmd7+z8zV3F4B4RpTdAIByEH4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GNLHJnR1mHj9LoIncJhLJH/9M+32u1rNtQ+M3sXEk/ktQm6Rfuvii1/iiN1ml2diO7BJCwzlfXvG7db/vNrE3STySdJ2mGpHlmNqPe5wNQrEY+88+S9LS7P+Pu+yTdLmlOPm0BaLZGwj9J0pZBj3uyZa9hZl1m1m1m3fu1t4HdAchTI+Ef6kuF1/0+2N0Xu3unu3e2q6OB3QHIUyPh75E0edDj4yRta6wdAEVpJPwPSZpuZseb2VGSLpK0Ip+2ADRb3UN97t5nZldI+oP6h/qWuvuTuXUGoKkaGud395WSVubUC4ACcXovEBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0EVOkU3jjwHP3Rqsn7xLfdVrN2wdG5y27df/2BdPaE2HPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiGxvnNbJOk3ZIOSOpz9848mkLr6L38zGR91bduSNaPbRtdsfaOS3+e3HbRqnnJuj/KjPCNyOMkn4+4+/M5PA+AAvG2Hwiq0fC7pPvN7GEz68qjIQDFaPRt/1nuvs3MxktaZWZ/c/c1g1fI/ih0SdIovbHB3QHIS0NHfnfflt32SrpH0qwh1lns7p3u3tmujkZ2ByBHdYffzEab2dED9yV9VNKGvBoD0FyNvO2fIOkeMxt4nlvd/fe5dAWg6eoOv7s/I+mUHHtBCZ677IxkvfvbNyfrbVZ5HL+arX1jk/URe/Yl6wfq3jMkhvqAsAg/EBThB4Ii/EBQhB8IivADQXHp7iPciFPenayvXnhjst5mzTsl+3fPvy9Z981bm7ZvcOQHwiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY5z/C/e3yMcn62LbGxvEv7Un/JPiT47or1m49/oHktictuCxZn/Kdtck60jjyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPMfAfzMyldQX/+JH1fZ+g3J6ksH9yTr/7ryXcn6174+rWJtw+nLk9vOmP10sv6/ZBXVcOQHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaCqjvOb2VJJ50vqdfeTs2XjJN0haaqkTZLmuvuu5rWJlP+8t/Jv8o8ZkR7Hr+ZjGz6TrI958LFk/ZVnT69cTJQk6TuTf5usf3vKp5L1vs1b0jsIrpYj/68knXvIsgWSVrv7dEmrs8cAhpGq4Xf3NZJ2HrJ4jqRl2f1lki7IuS8ATVbvZ/4J7r5dkrLb8fm1BKAITT+338y6JHVJ0ig1b943AIen3iP/DjObKEnZbW+lFd19sbt3untnuzrq3B2AvNUb/hWS5mf350u6N592ABSlavjN7DZJayWdaGY9ZnaJpEWSzjGzf0g6J3sMYBip+pnf3edVKJ2dcy+o04SLNjftud9w3Zsb2n7qiv0Vay986pXktjM70uco/Pe0Scn6GMb5kzjDDwiK8ANBEX4gKMIPBEX4gaAIPxAUl+4+Arxx5L66t12++y3J+sg/P56se5XnH/nHhyvW7th9QnLbrmO2Jeu7pqePXenJycGRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpx/GOi9/MxkfeW0myvWdh1IT7H9yy99Pllv63skWS/TnvEHy25hWOPIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc4/DLxw0oFkvc0q/w2/Y/f09LYPtO44PpqLIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFV1nN/Mlko6X1Kvu5+cLbtG0hclPZetttDdVzarSdRv3YvTqqyxu5A+0HpqOfL/StK5Qyz/gbvPzP4RfGCYqRp+d18jaWcBvQAoUCOf+a8ws8fNbKmZjc2tIwCFqDf8P5N0gqSZkrZLurHSimbWZWbdZta9X3vr3B2AvNUVfnff4e4H3P2gpFskzUqsu9jdO929s10d9fYJIGd1hd/MJg56eKGkDfm0A6AotQz13SZptqRjzaxH0nclzTazmeqfoXmTpEub2COAJqgafnefN8TiJU3oBRUsOOe+urd9cNXJyfpUra37uWsx4n0nVazNHPVQla3b08+9z+roCAM4ww8IivADQRF+ICjCDwRF+IGgCD8QFJfuHgZO6thWdgt123JNW8XarI70UN76venTwU+8uSdZ70tWwZEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4JinP8IN/Ypb+rzv3zhacn6qs6KV3iTNCa57fe2nJ+s923ekqwjjSM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOP8wcNmjn03WnzxjecXarhPTl7c+psq+2yaMT9a/ct3tyfrEkemx/JS//umdyfqUV2eIRz048gNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUFXH+c1ssqRfS3qbpIOSFrv7j8xsnKQ7JE2VtEnSXHff1bxW43pla/1j5RNm/TtZHzFzRrI+7ZZ/Jutzx7xw2D0NuP/l9HX7j1/0WLJ+sO49Q6rtyN8n6Wp3f7ek0yV92cxmSFogabW7T5e0OnsMYJioGn533+7uj2T3d0vaKGmSpDmSlmWrLZN0QbOaBJC/w/rMb2ZTJZ0qaZ2kCe6+Xer/AyEpfR4ogJZSc/jNbIykuyRd6e4vHsZ2XWbWbWbd+5Weew1AcWoKv5m1qz/4y9397mzxDjObmNUnSuodalt3X+zune7e2a6OPHoGkIOq4Tczk7RE0kZ3v2lQaYWk+dn9+ZLuzb89AM1Sy096z5J0saQnzGx9tmyhpEWS7jSzSyQ9K+nTzWkRjVjz3nvSK6wspo+hXLXki8n6cS8/WFAnMVUNv7v/RVKlH4WfnW87AIrCGX5AUIQfCIrwA0ERfiAowg8ERfiBoLh09zDwrm+sT9bPmP7JirW1p9yVdzuv8fi+Pcn656+9qmLtuCXr8m4Hh4EjPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTj/MOB705c/G/eFVyrWTv3pRcltH/1Aeorty7eenqw/teA9yfpbVq9N1lEejvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EJS5e2E7e5ON89OMq30DzbLOV+tF31npUvuvwZEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4KqGn4zm2xmD5jZRjN70sy+mi2/xsy2mtn67N/Hm98ugLzUcjGPPklXu/sjZna0pIfNbFVW+4G7f7957QFolqrhd/ftkrZn93eb2UZJk5rdGIDmOqzP/GY2VdKpkgbmWbrCzB43s6VmNrbCNl1m1m1m3fuVvhwVgOLUHH4zGyPpLklXuvuLkn4m6QRJM9X/zuDGobZz98Xu3unune3qyKFlAHmoKfxm1q7+4C9397slyd13uPsBdz8o6RZJs5rXJoC81fJtv0laImmju980aPnEQatdKGlD/u0BaJZavu0/S9LFkp4ws4G5ohdKmmdmMyW5pE2SLm1KhwCaopZv+/8iaajfB6/Mvx0AReEMPyAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCFTtFtZs9J2jxo0bGSni+sgcPTqr21al8SvdUrz96muPtba1mx0PC/budm3e7eWVoDCa3aW6v2JdFbvcrqjbf9QFCEHwiq7PAvLnn/Ka3aW6v2JdFbvUrprdTP/ADKU/aRH0BJSgm/mZ1rZk+Z2dNmtqCMHioxs01m9kQ283B3yb0sNbNeM9swaNk4M1tlZv/IboecJq2k3lpi5ubEzNKlvnatNuN14W/7zaxN0t8lnSOpR9JDkua5+18LbaQCM9skqdPdSx8TNrMPS3pJ0q/d/eRs2fWSdrr7ouwP51h3/2aL9HaNpJfKnrk5m1Bm4uCZpSVdIOlzKvG1S/Q1VyW8bmUc+WdJetrdn3H3fZJulzSnhD5anruvkbTzkMVzJC3L7i9T/3+ewlXorSW4+3Z3fyS7v1vSwMzSpb52ib5KUUb4J0naMuhxj1prym+XdL+ZPWxmXWU3M4QJ2bTpA9Onjy+5n0NVnbm5SIfMLN0yr109M17nrYzwDzX7TysNOZzl7u+XdJ6kL2dvb1GbmmZuLsoQM0u3hHpnvM5bGeHvkTR50OPjJG0roY8hufu27LZX0j1qvdmHdwxMkprd9pbcz6taaebmoWaWVgu8dq0043UZ4X9I0nQzO97MjpJ0kaQVJfTxOmY2OvsiRmY2WtJH1XqzD6+QND+7P1/SvSX28hqtMnNzpZmlVfJr12ozXpdykk82lPFDSW2Slrr7tYU3MQQzm6b+o73UP4nprWX2Zma3SZqt/l997ZD0XUm/kXSnpHdIelbSp9298C/eKvQ2W/1vXV+duXngM3bBvX1Q0p8lPSHpYLZ4ofo/X5f22iX6mqcSXjfO8AOC4gw/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB/R9GRL/uZKhJogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[3].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[3].view(-1, 784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
